# mkrivers

Generate river.js files from lists of feed URLs

## Install

$ git clone https://github.com/edavis/mkrivers
$ pip install -r mkrivers/requirements.pin

## Quick start

$ mkdir input/ output/
$ cat > input/news.txt
http://rss.nytimes.com/services/xml/rss/nyt/HomePage.xml
http://feeds.feedburner.com/TheAtlanticWire
http://feeds.theguardian.com/theguardian/world/usa/rss
http://feeds.reuters.com/reuters/topNews
http://news.yahoo.com/rss/
^D
$ mkrivers.py -o output/ input/

After running for a bit, you'll have a file ("output/news.js") that
you can rig up to be read by any library that understands the river.js
format.

## Changelog

- v0.5: Fix SSL warnings and include source name when logging.

  The ndg-httpsclient package was necessary for me to get HTTPS feeds
  working on Python 2.7.3. Your mileage may vary.

  Also, when logging feed fetch results, include the name of the source.

- v0.4: Include body text from feed.

- v0.3: Include `permaLink` in items and `aggregator` in
  metadata. Drop `From` request header.

- v0.2: Give each source its own cache.

  Previously, all feeds from all sources used a common cache to track
  already seen items. This caused a feed in multiple sources to only
  have new items reported in the source that crawled it first.

  Now, each source creates a sub-directory within RIVER_CACHE_DIR and
  individual feed caches are stored within it.

- v0.1: Initial release.

## TODO
- make it easy to create HTML pages
- customize riverBrowser.js
  - maybe use a proper JS frontend library?
- include easy way to navigate between different rivers
  - flask frontend with a JSON config file?
- can pass in a remote source list
- when pass dir for source input, also check for _new_ text files
- track counter in pickle, too
