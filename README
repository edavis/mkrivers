# mkrivers

Generate river.js files from lists of feed URLs

## Install

$ git clone https://github.com/edavis/mkrivers
$ pip install -r mkrivers/requirements.pin

## Quick start

$ mkdir input/ output/
$ cat > input/news.txt
http://rss.nytimes.com/services/xml/rss/nyt/HomePage.xml
http://feeds.feedburner.com/TheAtlanticWire
http://feeds.theguardian.com/theguardian/world/usa/rss
http://feeds.reuters.com/reuters/topNews
http://news.yahoo.com/rss/
^D
$ mkrivers.py -o output/ input/

After running for a bit, you'll have a file ("output/news.js") that
you can rig up to be read by any library that understands the river.js
format.

## Changelog

- v0.9: Add callback system.

  Callbacks can be used to do some feed-specific processing on new
  river items.

  Here's an example. Put something like this in `callbacks.py`:

    import uuid

    def change_id_to_uuid(url, entry, river_item):
      river_item['id'] = str(uuid.uuid4())

    item_callbacks = [change_id_to_uuid]

  Each callback function accepts three arguments: the URL of the feed,
  the new entry as processed by feedparser, and the river.js formatted
  object.

  river_item is passed by reference, so any changes to that object
  will be reflected on output.

  For each new entry found when a feed is parsed, mkrivers will cycle
  through each callback found in `item_callbacks`, in order.

- v0.8: Add remote source list support.

  You can now pass remote source lists and mkrivers will include any
  feeds it contains.

  To mark a URL as a remote source list in your local feed list, prefix
  it with an exclamation point:

  !http://www.remotesourcelist.com/list.txt
  http://regularfeed.com/rss

  If a remote source list contains another remote source list,
  mkrivers will fetch it. Make sure there isn't a circular dependency
  between remote source lists, otherwise mkrivers will go into an
  infinite loop.

- v0.7: Add/remove source lists as necessary and better handle feed errors.

  Also includes a handful of small fixes/improvements.

- v0.6: Notify user of broken feeds.

  When a feed's request error rate (defined as number of 4xx and 5xx
  HTTP status codes divided by its number of fetches) is greater than
  the configured threshold (80%), display a notice in the river.

- v0.5: Fix SSL warnings and include source name when logging.

  The ndg-httpsclient package was necessary for me to get HTTPS feeds
  working on Python 2.7.3. Your mileage may vary.

  Also, when logging feed fetch results, include the name of the source.

- v0.4: Include body text from feed.

- v0.3: Include `permaLink` in items and `aggregator` in
  metadata. Drop `From` request header.

- v0.2: Give each source its own cache.

  Previously, all feeds from all sources used a common cache to track
  already seen items. This caused a feed in multiple sources to only
  have new items reported in the source that crawled it first.

  Now, each source creates a sub-directory within RIVER_CACHE_DIR and
  individual feed caches are stored within it.

- v0.1: Initial release.

## TODO
- make it easy to create HTML pages
- customize riverBrowser.js
  - maybe use a proper JS frontend library?
- include easy way to navigate between different rivers
  - flask frontend with a JSON config file?
